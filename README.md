# pyML

机器学习课程作业代码

## 题目概览

本项目包含三个机器学习相关题目的实现与分析：

1. **基于支持向量机的分类算法设计与实现**
2. **线性回归（梯度下降法预测学生人数）**
3. **线性分类算法（如感知器或逻辑回归）**

---

## 题目一：基于支持向量机的分类算法设计与实现

- **数据集**：`wine_data.mat` 和 `wine_label.mat`
- **核心任务**：使用 SVM 对葡萄酒数据进行分类，并寻找最优参数。
- **具体要求**：
  - 利用 `wine_data.mat` 作为特征，`wine_label.mat` 作为标签，构建 SVM 分类模型。
  - 通过参数调优（如 C 和 gamma），寻找模型最优解。
  - 展示不同参数组合下的分类效果，并进行分析。
- **报告建议**：
  - **原理阐述**：详细讲解最大间隔、支持向量、核函数（线性核、多项式核、RBF 高斯核等），包括公式推导（如对偶问题求解）。
  - **实现与调优**：重点在于参数调优（如 C 和 gamma），建议使用 GridSearchCV 或 RandomizedSearchCV，展示不同参数组合下的性能对比。
  - **结果分析**：多维度评估模型（混淆矩阵、分类报告等），对比不同核函数的表现并分析原因。
- **推荐指数**：⭐⭐⭐⭐⭐
- **理由**：理论深度、实践操作和结果分析兼备，适合撰写高质量报告。

---

## 题目二：线性回归（梯度下降法预测学生人数）

- **数据集**：`stu_num.mat`（学生人数，单位：百人），`year.mat`（年份）
- **核心任务**：使用梯度下降法实现线性回归，预测学生人数。
- **具体要求**：
  - 利用 `year.mat` 和 `stu_num.mat` 数据，建立年份与学生人数的线性回归模型。
  - 使用梯度下降算法拟合参数。
  - 预测 2018 年和 2019 年学校的学生人数，并给出结果。
- **报告建议**：
  - **原理与推导**：推导损失函数（MSE）和参数更新公式，讨论 BGD、SGD、小批量梯度下降的区别。
  - **算法实现**：建议手动实现梯度下降（用 NumPy），展示核心代码。
  - **结果分析**：分析学习率对收敛速度和结果的影响，绘制损失函数曲线，讨论模型局限性。
- **推荐指数**：⭐⭐⭐⭐
- **理由**：适合深入理解梯度下降，报告深度取决于对优化算法的挖掘。

---

## 题目三：线性分类算法

- **数据集**：`positive.mat` 和 `negtive.mat`
- **核心任务**：实现线性分类器（如感知器或逻辑回归），区分正负样本。
- **具体要求**：
  - 使用 `positive.mat` 和 `negtive.mat` 两个样本数据，构建线性分类模型。
  - 采用课堂讲授的线性分类算法进行实现。
  - 展示分类结果，并可视化决策边界。
- **报告建议**：
  - **算法选择**：明确所用算法（如感知器），阐述其原理。
  - **可视化**：对二维数据进行可视化，展示决策边界。
  - **手动实现**：建议手动实现算法（如感知器或逻辑回归的梯度下降）。
- **推荐指数**：⭐⭐⭐
- **理由**：实现简单，可视化效果好，适合时间有限或对几何意义感兴趣的同学。

---

## 总结

- **SVM 分类**：理论与实践兼备，最推荐用于写深度报告。
- **线性回归**：适合深入理解梯度下降，报告深度可控。
- **线性分类**：实现简单，适合可视化展示。

欢迎根据自身兴趣和时间选择合适的题目进行深入研究与实现。
